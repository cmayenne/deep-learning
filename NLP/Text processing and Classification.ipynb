{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVEEazxFZgr2",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-Load-the-data\" data-toc-modified-id=\"1.-Load-the-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>1. Load the data</a></span></li><li><span><a href=\"#2.-Filtering-out-the-noise\" data-toc-modified-id=\"2.-Filtering-out-the-noise-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>2. Filtering out the noise</a></span></li><li><span><a href=\"#3.-Even-better-filtering\" data-toc-modified-id=\"3.-Even-better-filtering-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>3. Even better filtering</a></span></li><li><span><a href=\"#4.-Term-frequency-times-inverse-document-frequency\" data-toc-modified-id=\"4.-Term-frequency-times-inverse-document-frequency-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>4. Term frequency times inverse document frequency</a></span></li><li><span><a href=\"#5.-Utility-function\" data-toc-modified-id=\"5.-Utility-function-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>5. Utility function</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjpQefesZgr4"
   },
   "source": [
    "This notebook is part of the [Machine Learning class](https://github.com/erachelson/MLclass) by [Emmanuel Rachelson](https://personnel.isae-supaero.fr/emmanuel-rachelson?lang=en).\n",
    "\n",
    "License: CC-BY-SA-NC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1VXs8S8Zgr7"
   },
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">Text data pre-processing</div>\n",
    "\n",
    "In this exercice, we shall load a database of email messages and pre-format them so that we can design automated classification methods or use off-the-shelf classifiers.\n",
    "\n",
    "\"What is there to pre-process?\" you might ask. Well, actually, text data comes in a very noisy form that we, humans, have become accustomed to and filter out effortlessly to grasp the core meaning of the text. It has a lot of formatting (fonts, colors, typography...), punctuation, abbreviations, common words, grammatical rules, etc. that we might wish to discard before even starting the data analysis.\n",
    "\n",
    "Here are some pre-processing steps that can be performed on text:\n",
    "1. loading the data, removing attachements, merging title and body;\n",
    "2. tokenizing - splitting the text into atomic \"words\";\n",
    "3. removal of stop-words - very common words;\n",
    "4. removal of non-words - punctuation, numbers, gibberish;\n",
    "3. lemmatization - merge together \"find\", \"finds\", \"finder\".\n",
    "\n",
    "The final goal is to be able to represent a document as a mathematical object, e.g. a vector, that our machine learning black boxes can process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGMK6qekZgr9"
   },
   "source": [
    "# 1. Text classification in English\n",
    "\n",
    "## 1.1 Load the data\n",
    "\n",
    "Let's first load the emails."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of emails 2893\n",
      "email file: ../data/lingspam_public/bare/part7/6-255msg3.txt\n",
      "email is a spam: False\n",
      "Subject: dick armey 's slip\n",
      "\n",
      "so dave wharton , having determined that the delay between richard armey 's \" barney fag \" remark and its correction lasted less than a second , is confident that it must have been \" a slip and not a slur \" ( presumably blameless ) , all the more since armey has a ph . d . and a wealth of political savvy and \" would not think it to his advantage to make such an utterance . \" a victim , then , of linguists with ulterior agendas . how in the in the world is one to respond to such a statement ? if wharton had made the suggestion 75 years ago you might repeat the observation that freud offered in the introductory lectures to the effect that the merely somatic or phonetic concommitents of slips can't explain why they occur when they do - - as he put it , it 's like telling a policeman that the darkness of the night and the isolation of the street have caused your purse to be snatched . what might have possessed armey , then ? the new republic has pointed out that he was one of only forty-seven members ( gingrich was not among them ) who voted against george bush 's hate crime statistics act , which allowed the government to record violence against homosexuals ; that he voted to exclude people with aids from the americans with disabilities act ; that he voted to deny government funds to groups that boycotted the boy scouts of america on the grounds of that organization 's anti-gay policies ; and that he refused to sign a voluntary statement saying that his own office did n't discriminate against homosexuals . of course the remark was n't \" intentional , \" but the evidence is pretty thick that armey harbors just the sorts of inner demons who would have been lying in wait for any breach in conscious attention . most inhabitants of the late 20th century will acknowlege some acquaintance with pesky creatures like these , and you would think that it would be only by an act of willful repression that someone could deny their existence entirely . but maybe we should give wharton the benefit of the doubt ; maybe his is a genuine victorian innocence . only , just think of it ! all those theorists arguing that we are living the twilight of modernist era , when there are still people ( with an \" edu \" in their address , yet ) on whom it has not even begun to dawn .\n",
      "\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "id": "26qQSlBNZpZY"
   },
   "outputs": [],
   "source": [
    "#!git clone https://github.com/SupaeroDataScience/deep-learning\n",
    "#!mv deep-learning/data .\n",
    "#!mv deep-learning/NLP/datasets .\n",
    "#!pip install nltk unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYwNdKZiZgr9"
   },
   "outputs": [],
>>>>>>> a74126e1bd138ec40f9ec59bad7be159baf0fceb
   "source": [
    "import os\n",
    "data_switch=1\n",
    "if(data_switch==0):\n",
    "    train_dir = 'data/ling-spam/train-mails/'\n",
    "    email_path = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]\n",
    "else:\n",
    "    train_dir = 'data/lingspam_public/bare/'\n",
    "    email_path = []\n",
    "    email_label = []\n",
    "    for d in os.listdir(train_dir):\n",
    "        folder = os.path.join(train_dir,d)\n",
    "        email_path += [os.path.join(folder,f) for f in os.listdir(folder)]\n",
    "        email_label += [f[0:3]=='spm' for f in os.listdir(folder)]\n",
    "print(\"number of emails\",len(email_path))\n",
    "email_nb = 8 # try 8 for a spam example\n",
    "print(\"email file:\", email_path[email_nb])\n",
    "print(\"email is a spam:\", email_label[email_nb])\n",
    "print(open(email_path[email_nb]).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBPuFTw2ZgsA"
   },
   "source": [
    "## 1.2. Filtering out the noise\n",
    "\n",
    "One nice thing about scikit-learn is that is has lots of preprocessing utilities. Like [`CountVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) for instance, that converts a collection of text documents to a matrix of token counts.\n",
    "\n",
    "- To remove stop-words, we set: `stop_words='english'`\n",
    "- To convert all words to lowercase: `lowercase=True`\n",
    "- The default tokenizer in scikit-learn removes punctuation and only keeps words of more than 2 letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKLxbfC-ZgsB"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "countvect = CountVectorizer(input='filename', stop_words='english', lowercase=True)\n",
    "word_count = countvect.fit_transform(email_path)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2893\n",
      "Number of words: 60618\n",
      "Document - words matrix: (2893, 60618)\n",
      "First words: ['00', '000', '0000', '00001', '00003000140', '00003003958', '00007', '0001', '00010', '00014', '0003', '00036', '000bp', '000s', '000yen', '001', '0010', '0010010034', '0011', '00133', '0014', '00170', '0019', '00198', '002', '002656', '0027', '003', '0030', '0031', '00333', '0037', '0039', '003n7', '004', '0041', '0044', '0049', '005', '0057', '006', '0067', '007', '00710', '0073', '0074', '00799', '008', '009', '00919680', '0094', '00a', '00am', '00arrival', '00b', '00coffee', '00congress', '00d', '00dinner', '00f', '00h', '00hfstahlke', '00i', '00j', '00l', '00m', '00p', '00pm', '00r', '00t', '00tea', '00the', '00uzheb', '01', '0100', '01003', '01006', '0104', '0106', '01075', '0108', '011', '0111', '0117', '0118', '01202', '01222', '01223', '01225', '01232', '01235', '01273', '013', '0131', '01334', '0135', '01364', '0139', '013953', '013a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charles-alexis/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "id": "8eLR5hi3ZgsC"
   },
   "outputs": [],
>>>>>>> a74126e1bd138ec40f9ec59bad7be159baf0fceb
   "source": [
    "print(\"Number of documents:\", len(email_path))\n",
    "words = countvect.get_feature_names()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", word_count.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7H9GRQxXZgsC"
   },
   "source": [
    "## 1.3. Even better filtering\n",
    "\n",
    "That's already quite ok, but this pre-processing does not perform lemmatization, the list of stop-words could be better and we could wish to remove non-english words (misspelled, with numbers, etc.).\n",
    "\n",
    "A slightly better preprocessing uses the [Natural Language Toolkit](https://www.nltk.org/https://www.nltk.org/). The one below:\n",
    "- tokenizes;\n",
    "- removes punctuation;\n",
    "- removes stop-words;\n",
    "- removes non-English and misspelled words (optional);\n",
    "- removes 1-character words;\n",
    "- removes non-alphabetical words (numbers and codes essentially)."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/charles-\n",
      "[nltk_data]     alexis/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/charles-\n",
      "[nltk_data]     alexis/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/charles-\n",
      "[nltk_data]     alexis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "id": "qbwk9Mp0ZgsD"
   },
   "outputs": [],
>>>>>>> a74126e1bd138ec40f9ec59bad7be159baf0fceb
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLBm1kLIZgsE"
   },
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self, remove_non_words=True):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.words = set(words.words())\n",
    "        self.remove_non_words = remove_non_words\n",
    "    def __call__(self, doc):\n",
    "        # tokenize words and punctuation\n",
    "        word_list = wordpunct_tokenize(doc)\n",
    "        # remove stopwords\n",
    "        word_list = [word for word in word_list if word not in self.stopwords]\n",
    "        # remove non words\n",
    "        if(self.remove_non_words):\n",
    "            word_list = [word for word in word_list if word in self.words]\n",
    "        # remove 1-character words\n",
    "        word_list = [word for word in word_list if len(word)>1]\n",
    "        # remove non alpha\n",
    "        word_list = [word for word in word_list if word.isalpha()]\n",
    "        return [self.wnl.lemmatize(t) for t in word_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXUNGD9AZgsG"
   },
   "source": [
    "The LemmaTokenizer defined above will be applied further in this example. The next step is to define the Count Vectorization pipeline using this Tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sa66gFMlZgsG"
   },
   "outputs": [],
   "source": [
    "countvect = CountVectorizer(input='filename',tokenizer=LemmaTokenizer(remove_non_words=True))\n",
    "bow = countvect.fit_transform(email_path)\n",
    "feat2word = {v: k for k, v in countvect.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IIKuiDWSZgsH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of documents:\", len(email_path))\n",
    "words = countvect.get_feature_names()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", bow.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cb-3Z74eZgsI"
   },
   "source": [
    "## 1.4. Using the bag of words (BOW) object to classify spam\n",
    "\n",
    "Let's start by splitting the data into train and test sets, using 20% of the data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gf-54hwSZgsI"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow,email_label,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rS7EDB-ZgsI"
   },
   "source": [
    "In this simple example we will use a Logistic Regression Classifier. Let's fit it to our Training Data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9861830742659758\n",
      "Precision : 0.9770114942528736\n",
      "Recall : 0.9340659340659341\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "id": "BxD1T5OiZgsJ"
   },
   "outputs": [],
>>>>>>> a74126e1bd138ec40f9ec59bad7be159baf0fceb
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "lr_classifier=LogisticRegression()\n",
    "lr_classifier.fit(X_train,y_train)\n",
    "\n",
    "y_predicted = lr_classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test,y_predicted))\n",
    "print(\"Precision :\",metrics.precision_score(y_test,y_predicted))\n",
    "print(\"Recall :\",metrics.recall_score(y_test,y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKsqSjg0ZgsJ"
   },
   "source": [
    "In many cases, Bag of Words can provide sufficient information for classification. In this case, the accuracy reached by our classifier is pretty good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "naewZmg0ZgsK"
   },
   "source": [
    "## 1.5. Term frequency times inverse document frequency\n",
    "\n",
    "After this first preprocessing, each document is summarized by a vector of size \"number of words in the extracted dictionnary\". For example, the first email in the list has become:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s9imM6DqZgsK",
    "scrolled": false
   },
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original email:\n",
      "Subject: inquiry re : slang and rock music\n",
      "\n",
      "i am working on a project where i need to explore the relationship between rock & roll music and current slang . any suggestions on previous studies or books or any contributions would be greatly appreciated . i will post a summary and e - mail directly to anyone who requests a follow up . thank you carolyn chriss caroza @ aol . com\n",
      "\n",
      "Bag of words representation (22 words in dict):\n",
      "{'subject': 1, 'inquiry': 1, 'slang': 2, 'rock': 2, 'music': 2, 'working': 1, 'project': 1, 'need': 1, 'explore': 1, 'relationship': 1, 'roll': 1, 'current': 1, 'previous': 1, 'would': 1, 'greatly': 1, 'post': 1, 'summary': 1, 'mail': 1, 'directly': 1, 'anyone': 1, 'follow': 1, 'thank': 1}\n",
      "\n",
      "Vector reprensentation (22 non-zero elements):\n",
      "  (0, 12153)\t1\n",
      "  (0, 6443)\t1\n",
      "  (0, 11558)\t2\n",
      "  (0, 10808)\t2\n",
      "  (0, 8136)\t2\n",
      "  (0, 14171)\t1\n",
      "  (0, 9803)\t1\n",
      "  (0, 8251)\t1\n",
      "  (0, 4497)\t1\n",
      "  (0, 10422)\t1\n",
      "  (0, 10821)\t1\n",
      "  (0, 3025)\t1\n",
      "  (0, 9695)\t1\n",
      "  (0, 14188)\t1\n",
      "  (0, 5491)\t1\n",
      "  (0, 9499)\t1\n",
      "  (0, 12266)\t1\n",
      "  (0, 7486)\t1\n",
      "  (0, 3525)\t1\n",
      "  (0, 614)\t1\n",
      "  (0, 4922)\t1\n",
      "  (0, 12708)\t1\n"
     ]
    }
   ],
=======
   "outputs": [],
>>>>>>> a74126e1bd138ec40f9ec59bad7be159baf0fceb
   "source": [
    "mail_number = 0\n",
    "text = open(email_path[mail_number]).read()\n",
    "print(\"Original email:\")\n",
    "print(text)\n",
    "\n",
    "emailBagOfWords = {feat2word[i]: bow[mail_number, i] for i in bow[mail_number, :].nonzero()[1]}\n",
    "print(\"Bag of words representation (\", len(emailBagOfWords), \" words in dict):\", sep='')\n",
    "print(emailBagOfWords)\n",
    "print(\"\\nVector reprensentation (\", bow[mail_number, :].nonzero()[1].shape[0], \" non-zero elements):\", sep='')\n",
    "print(bow[mail_number, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCV1BC1RZgsL"
   },
   "source": [
    "Counting words is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "\n",
    "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called `tf` for Term Frequencies.\n",
    "\n",
    "Another refinement on top of `tf` is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "This downscaling is called `tf–idf` for “Term Frequency times Inverse Document Frequency” and again, scikit-learn does the job for us with the [TfidfTransformer](scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNuCQwo8ZgsL"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer().fit_transform(bow)\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhKhbIoeZgsM"
   },
   "source": [
    "Let's run the classification process again"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9602763385146805\n",
      "Precision : 1.0\n",
      "Recall : 0.7553191489361702\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "id": "-NkGrarpZgsM"
   },
   "outputs": [],
>>>>>>> a74126e1bd138ec40f9ec59bad7be159baf0fceb
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf,email_label,test_size=0.2)\n",
    "\n",
    "#Fitting classifier\n",
    "lr_classifier.fit(X_train,y_train)\n",
    "\n",
    "#Testing classifier\n",
    "y_predicted = lr_classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test,y_predicted))\n",
    "print(\"Precision :\",metrics.precision_score(y_test,y_predicted))\n",
    "print(\"Recall :\",metrics.recall_score(y_test,y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_icBmOuZgsM"
   },
   "source": [
    "In this simplae case, additional filtering is unecessary and even removed some information. There is indeed likely a link between the abundance of words/long emails and the fact that this email is a spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcZVAEwsZgsN"
   },
   "source": [
    "# 2. Text classification in French\n",
    "\n",
    "The previously used dataset is a widely used dataset for introductory text classification. \n",
    "\n",
    "The field of Natural Language Understanding, and Natural Language Classification in particular, suffers from two challenges :\n",
    "- Adapting the features and methodologies to various and more complex datasets\n",
    "- Adapting the process to languages other than english\n",
    "\n",
    "Concerning the latter, one has to take into account that most of NLU research is currently performed on english. Datasets are rarely available for other languages, and the algorithms proposed for better NLU are often left untested on foreign data. \n",
    "French, for instance, has less efficient lemmatization (french is a richly flected language). In the following section, we will reuse the same methodologies on a french dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4TzD5FKrZgsN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#load video games reviews\n",
    "vgr = pd.read_csv(\"datasets/jvc.csv\")\n",
    "vgr.head()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAGdCAYAAADzOWwgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAAApZklEQVR4nO3deXRUdZ7//1dBFrZUwmISMmxpWSOLQ2yhWu0ZIE2AtA2C06AoQaI2dHCAiCynbWi1zwRhRKFBcEYgcGxFOAPakgGMAUK3RJZAZFEj2khgkkpoMSlAspDc3x/+Ul+LQCBFkkr4PB/n1DnUvZ/61PvDh3vqxafuvWWzLMsSAACAwZr5ugAAAABfIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIzn5+sCmoLKykrl5eUpKChINpvN1+UAAICbYFmWLly4oIiICDVrVvMaEIHoJuTl5alz586+LgMAAHjhzJkz6tSpU41tCEQ3ISgoSNIPf6F2u93H1QAAgJvhcrnUuXNn9+d4TQhEN6HqazK73U4gAgCgibmZ0104qRoAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeH6+LgAAYI5u81J9XUKtfbMoztcloAGwQgQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPD9fFwAAqL1u81J9XQJwW2GFCAAAGI9ABAAAjNdoAtGiRYtks9k0c+ZM97aSkhIlJiaqffv2atOmjcaNG6eCggKP1+Xm5iouLk6tWrVSaGionnvuOV25csWjzZ49ezRw4EAFBgaqe/fuSklJaYARAQCApqJRBKKDBw/qjTfeUP/+/T22z5o1Sx988IE2b96sjIwM5eXlaezYse79FRUViouLU1lZmfbt26f169crJSVFCxYscLc5deqU4uLiNGTIEGVnZ2vmzJl68skntXPnzgYbHwAAaNx8HoguXryoiRMn6r//+7/Vtm1b9/bi4mKtWbNGS5cu1dChQxUdHa1169Zp3759+uSTTyRJH374oT777DO99dZbuvvuuzVy5Ei99NJLWrlypcrKyiRJq1evVmRkpF555RX16dNH06dP18MPP6xXX33VJ+MFAACNj88DUWJiouLi4hQTE+OxPSsrS+Xl5R7be/furS5duigzM1OSlJmZqX79+iksLMzdJjY2Vi6XSydOnHC3ubrv2NhYdx/XUlpaKpfL5fEAAAC3L59edr9x40YdPnxYBw8erLbP6XQqICBAISEhHtvDwsLkdDrdbX4chqr2V+2rqY3L5dLly5fVsmXLau+dnJysF154wetxAQCApsVnK0RnzpzRjBkz9Oc//1ktWrTwVRnXNH/+fBUXF7sfZ86c8XVJAACgHvksEGVlZamwsFADBw6Un5+f/Pz8lJGRoeXLl8vPz09hYWEqKytTUVGRx+sKCgoUHh4uSQoPD6921VnV8xu1sdvt11wdkqTAwEDZ7XaPBwAAuH35LBANGzZMx44dU3Z2tvtxzz33aOLEie4/+/v7Kz093f2anJwc5ebmyuFwSJIcDoeOHTumwsJCd5u0tDTZ7XZFRUW52/y4j6o2VX0AAAD47ByioKAg9e3b12Nb69at1b59e/f2hIQEJSUlqV27drLb7XrmmWfkcDg0ePBgSdLw4cMVFRWlxx9/XIsXL5bT6dTzzz+vxMREBQYGSpKmTp2qFStWaM6cOZoyZYp27dqlTZs2KTWV294DAIAfNOrfMnv11VfVrFkzjRs3TqWlpYqNjdXrr7/u3t+8eXNt27ZN06ZNk8PhUOvWrRUfH68XX3zR3SYyMlKpqamaNWuWli1bpk6dOunNN99UbGysL4YEAAAaIZtlWZavi2jsXC6XgoODVVxczPlEABoFfty14XyzKM7XJcBLtfn89vl9iAAAAHyNQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACM59NAtGrVKvXv3192u112u10Oh0Pbt2937y8pKVFiYqLat2+vNm3aaNy4cSooKPDoIzc3V3FxcWrVqpVCQ0P13HPP6cqVKx5t9uzZo4EDByowMFDdu3dXSkpKQwwPAAA0ET4NRJ06ddKiRYuUlZWlQ4cOaejQoRo9erROnDghSZo1a5Y++OADbd68WRkZGcrLy9PYsWPdr6+oqFBcXJzKysq0b98+rV+/XikpKVqwYIG7zalTpxQXF6chQ4YoOztbM2fO1JNPPqmdO3c2+HgBAEDjZLMsy/J1ET/Wrl07LVmyRA8//LDuuOMOvf3223r44YclSV988YX69OmjzMxMDR48WNu3b9cvf/lL5eXlKSwsTJK0evVqzZ07V+fOnVNAQIDmzp2r1NRUHT9+3P0eEyZMUFFRkXbs2HFTNblcLgUHB6u4uFh2u73uBw0AtdRtXqqvSzDGN4vifF0CvFSbz+9Gcw5RRUWFNm7cqEuXLsnhcCgrK0vl5eWKiYlxt+ndu7e6dOmizMxMSVJmZqb69evnDkOSFBsbK5fL5V5lyszM9Oijqk1VH9dSWloql8vl8QAAALcvnweiY8eOqU2bNgoMDNTUqVO1detWRUVFyel0KiAgQCEhIR7tw8LC5HQ6JUlOp9MjDFXtr9pXUxuXy6XLly9fs6bk5GQFBwe7H507d66LoQIAgEbK54GoV69eys7O1v79+zVt2jTFx8frs88+82lN8+fPV3Fxsftx5swZn9YDAADql5+vCwgICFD37t0lSdHR0Tp48KCWLVum8ePHq6ysTEVFRR6rRAUFBQoPD5ckhYeH68CBAx79VV2F9uM2V1+ZVlBQILvdrpYtW16zpsDAQAUGBtbJ+AAAQOPn8xWiq1VWVqq0tFTR0dHy9/dXenq6e19OTo5yc3PlcDgkSQ6HQ8eOHVNhYaG7TVpamux2u6KiotxtftxHVZuqPgAAAHy6QjR//nyNHDlSXbp00YULF/T2229rz5492rlzp4KDg5WQkKCkpCS1a9dOdrtdzzzzjBwOhwYPHixJGj58uKKiovT4449r8eLFcjqdev7555WYmOhe4Zk6dapWrFihOXPmaMqUKdq1a5c2bdqk1FSu0AAAAD/waSAqLCzUpEmTlJ+fr+DgYPXv3187d+7UL37xC0nSq6++qmbNmmncuHEqLS1VbGysXn/9dffrmzdvrm3btmnatGlyOBxq3bq14uPj9eKLL7rbREZGKjU1VbNmzdKyZcvUqVMnvfnmm4qNjW3w8QIAgMap0d2HqDHiPkQAGhvuQ9RwuA9R09Uk70MEAADgKwQiAABgPAIRAAAwHoEIAAAYj0AEAACM51Ug+vvf/17XdQAAAPiMV4Goe/fuGjJkiN566y2VlJTUdU0AAAANyqtAdPjwYfXv319JSUkKDw/Xb37zm2q/KQYAANBUeBWI7r77bi1btkx5eXlau3at8vPzdf/996tv375aunSpzp07V9d1AgAA1JtbOqnaz89PY8eO1ebNm/Xyyy/rq6++0uzZs9W5c2f3T3IAAAA0drcUiA4dOqTf/va36tixo5YuXarZs2fr66+/VlpamvLy8jR69Oi6qhMAAKDeePXjrkuXLtW6deuUk5OjUaNGacOGDRo1apSaNfshX0VGRiolJUXdunWry1oBAADqhVeBaNWqVZoyZYomT56sjh07XrNNaGio1qxZc0vFAQAANASvAtHJkydv2CYgIEDx8fHedA8AANCgvDqHaN26ddq8eXO17Zs3b9b69etvuSgAAICG5FUgSk5OVocOHaptDw0N1X/8x3/cclEAAAANyatAlJubq8jIyGrbu3btqtzc3FsuCgAAoCF5FYhCQ0N19OjRats//fRTtW/f/paLAgAAaEheBaJHHnlE//7v/67du3eroqJCFRUV2rVrl2bMmKEJEybUdY0AAAD1yqurzF566SV98803GjZsmPz8fuiisrJSkyZN4hwiAADQ5HgViAICAvTuu+/qpZde0qeffqqWLVuqX79+6tq1a13XBwAAUO+8CkRVevbsqZ49e9ZVLQAAAD7hVSCqqKhQSkqK0tPTVVhYqMrKSo/9u3btqpPiAAAAGoJXgWjGjBlKSUlRXFyc+vbtK5vNVtd1AQAANBivAtHGjRu1adMmjRo1qq7rAQAAaHBeXXYfEBCg7t2713UtAAAAPuHVCtGzzz6rZcuWacWKFXxdBqDJ6zYv1dclAPAxrwLR3/72N+3evVvbt2/XXXfdJX9/f4/9W7ZsqZPiAAAAGoJXgSgkJEQPPfRQXdcCAADgE14FonXr1tV1HQAAAD7j1UnVknTlyhV99NFHeuONN3ThwgVJUl5eni5evFhnxQEAADQEr1aITp8+rREjRig3N1elpaX6xS9+oaCgIL388ssqLS3V6tWr67pOAAB8oimedP/Nojhfl9DkeLVCNGPGDN1zzz367rvv1LJlS/f2hx56SOnp6XVWHAAAQEPwaoXor3/9q/bt26eAgACP7d26ddP//d//1UlhAAAADcWrFaLKykpVVFRU23727FkFBQXdclEAAAANyatANHz4cL322mvu5zabTRcvXtTChQv5OQ8AANDkePWV2SuvvKLY2FhFRUWppKREjz76qE6ePKkOHTronXfeqesaAQAA6pVXgahTp0769NNPtXHjRh09elQXL15UQkKCJk6c6HGSNQAAQFPgVSCSJD8/Pz322GN1WQsAAIBPeBWINmzYUOP+SZMmeVUMAACAL3gViGbMmOHxvLy8XN9//70CAgLUqlUrAhEAAGhSvLrK7LvvvvN4XLx4UTk5Obr//vs5qRoAADQ5Xv+W2dV69OihRYsWVVs9AgAAaOzqLBBJP5xonZeXV5ddAgAA1DuvziH6y1/+4vHcsizl5+drxYoVuu++++qkMAAAgIbiVSAaM2aMx3ObzaY77rhDQ4cO1SuvvFIXdQEAADQYrwJRZWVlXdcBAADgM3V6DhEAAEBT5NUKUVJS0k23Xbp0qTdvAQAA0GC8CkRHjhzRkSNHVF5erl69ekmSvvzySzVv3lwDBw50t7PZbHVTJQAAQD3yKhA9+OCDCgoK0vr169W2bVtJP9ys8YknntADDzygZ599tk6LBAAAqE9enUP0yiuvKDk52R2GJKlt27b64x//yFVmAACgyfEqELlcLp07d67a9nPnzunChQu3XBQAAEBD8ioQPfTQQ3riiSe0ZcsWnT17VmfPntX//M//KCEhQWPHjq3rGgEAAOqVV+cQrV69WrNnz9ajjz6q8vLyHzry81NCQoKWLFlSpwUCAADUN68CUatWrfT6669ryZIl+vrrryVJd955p1q3bl2nxQEAADSEW7oxY35+vvLz89WjRw+1bt1almXVVV0AAAANxqtA9O2332rYsGHq2bOnRo0apfz8fElSQkICl9wDAIAmx6tANGvWLPn7+ys3N1etWrVybx8/frx27NhRZ8UBAAA0BK/OIfrwww+1c+dOderUyWN7jx49dPr06TopDAAAoKF4tUJ06dIlj5WhKufPn1dgYOAtFwUAANCQvApEDzzwgDZs2OB+brPZVFlZqcWLF2vIkCF1VhwAAEBD8Oors8WLF2vYsGE6dOiQysrKNGfOHJ04cULnz5/Xxx9/XNc1AgAA1CuvVoj69u2rL7/8Uvfff79Gjx6tS5cuaezYsTpy5IjuvPPOuq4RAACgXtV6hai8vFwjRozQ6tWr9bvf/a4+agIAAGhQtV4h8vf319GjR+vkzZOTk/XTn/5UQUFBCg0N1ZgxY5STk+PRpqSkRImJiWrfvr3atGmjcePGqaCgwKNNbm6u4uLi1KpVK4WGhuq5557TlStXPNrs2bNHAwcOVGBgoLp3766UlJQ6GQMAAGj6vPrK7LHHHtOaNWtu+c0zMjKUmJioTz75RGlpaSovL9fw4cN16dIld5tZs2bpgw8+0ObNm5WRkaG8vDyPH5CtqKhQXFycysrKtG/fPq1fv14pKSlasGCBu82pU6cUFxenIUOGKDs7WzNnztSTTz6pnTt33vIYAABA02ezvPi9jWeeeUYbNmxQjx49FB0dXe03zJYuXepVMefOnVNoaKgyMjL085//XMXFxbrjjjv09ttv6+GHH5YkffHFF+rTp48yMzM1ePBgbd++Xb/85S+Vl5ensLAwST/8+OzcuXN17tw5BQQEaO7cuUpNTdXx48fd7zVhwgQVFRXd1I0kXS6XgoODVVxcLLvd7tXYADRe3eal+roEoE59syjO1yU0CrX5/K7VCtHf//53VVZW6vjx4xo4cKCCgoL05Zdf6siRI+5Hdna214UXFxdLktq1aydJysrKUnl5uWJiYtxtevfurS5duigzM1OSlJmZqX79+rnDkCTFxsbK5XLpxIkT7jY/7qOqTVUfVystLZXL5fJ4AACA21etTqru0aOH8vPztXv3bkk//FTH8uXLPcKItyorKzVz5kzdd9996tu3ryTJ6XQqICBAISEhHm3DwsLkdDrdba5+/6rnN2rjcrl0+fJltWzZ0mNfcnKyXnjhhVseEwAAaBpqtUJ09bdr27dv9zjf51YkJibq+PHj2rhxY530dyvmz5+v4uJi9+PMmTO+LgkAANQjr27MWMWL04+uafr06dq2bZv27t3r8fto4eHhKisrU1FRkccqUUFBgcLDw91tDhw44NFf1VVoP25z9ZVpBQUFstvt1VaHJCkwMJCfIAEAwCC1WiGy2Wyy2WzVtnnLsixNnz5dW7du1a5duxQZGemxPzo6Wv7+/kpPT3dvy8nJUW5urhwOhyTJ4XDo2LFjKiwsdLdJS0uT3W5XVFSUu82P+6hqU9UHAAAwW61WiCzL0uTJk92rJyUlJZo6dWq1q8y2bNlyU/0lJibq7bff1vvvv6+goCD3OT/BwcFq2bKlgoODlZCQoKSkJLVr1052u13PPPOMHA6HBg8eLEkaPny4oqKi9Pjjj2vx4sVyOp16/vnnlZiY6K5z6tSpWrFihebMmaMpU6Zo165d2rRpk1JTubIEAADUMhDFx8d7PH/sscdu6c1XrVolSfrXf/1Xj+3r1q3T5MmTJUmvvvqqmjVrpnHjxqm0tFSxsbF6/fXX3W2bN2+ubdu2adq0aXI4HGrdurXi4+P14osvuttERkYqNTVVs2bN0rJly9SpUye9+eabio2NvaX6AQDA7cGr+xCZhvsQAbc37kOE2w33IfpBvd2HCAAA4HZEIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwnp+vCwBwe+k2L9XXJQBArbFCBAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMbz83UBAK6v27xUX5cAAEZghQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDyfBqK9e/fqwQcfVEREhGw2m9577z2P/ZZlacGCBerYsaNatmypmJgYnTx50qPN+fPnNXHiRNntdoWEhCghIUEXL170aHP06FE98MADatGihTp37qzFixfX99AAAEAT4tNAdOnSJQ0YMEArV6685v7Fixdr+fLlWr16tfbv36/WrVsrNjZWJSUl7jYTJ07UiRMnlJaWpm3btmnv3r16+umn3ftdLpeGDx+url27KisrS0uWLNEf/vAH/dd//Ve9jw8AADQNNsuyLF8XIUk2m01bt27VmDFjJP2wOhQREaFnn31Ws2fPliQVFxcrLCxMKSkpmjBhgj7//HNFRUXp4MGDuueeeyRJO3bs0KhRo3T27FlFRERo1apV+t3vfien06mAgABJ0rx58/Tee+/piy++uKnaXC6XgoODVVxcLLvdXveDB66DO1UD8MY3i+J8XUKjUJvP70Z7DtGpU6fkdDoVExPj3hYcHKxBgwYpMzNTkpSZmamQkBB3GJKkmJgYNWvWTPv373e3+fnPf+4OQ5IUGxurnJwcfffdd9d879LSUrlcLo8HAAC4fTXaQOR0OiVJYWFhHtvDwsLc+5xOp0JDQz32+/n5qV27dh5trtXHj9/jasnJyQoODnY/OnfufOsDAgAAjVajDUS+NH/+fBUXF7sfZ86c8XVJAACgHjXaQBQeHi5JKigo8NheUFDg3hceHq7CwkKP/VeuXNH58+c92lyrjx+/x9UCAwNlt9s9HgAA4PbVaANRZGSkwsPDlZ6e7t7mcrm0f/9+ORwOSZLD4VBRUZGysrLcbXbt2qXKykoNGjTI3Wbv3r0qLy93t0lLS1OvXr3Utm3bBhoNAABozHwaiC5evKjs7GxlZ2dL+uFE6uzsbOXm5spms2nmzJn64x//qL/85S86duyYJk2apIiICPeVaH369NGIESP01FNP6cCBA/r44481ffp0TZgwQREREZKkRx99VAEBAUpISNCJEyf07rvvatmyZUpKSvLRqAEAQGPj58s3P3TokIYMGeJ+XhVS4uPjlZKSojlz5ujSpUt6+umnVVRUpPvvv187duxQixYt3K/585//rOnTp2vYsGFq1qyZxo0bp+XLl7v3BwcH68MPP1RiYqKio6PVoUMHLViwwONeRQAAwGyN5j5EjRn3IYKvcB8iAN7gPkQ/uC3uQwQAANBQCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwnp+vCwAaSrd5qb4uAQDQSLFCBAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPD9fF4Cmqdu8VF+XAABAnWGFCAAAGI8VIgAAbjNNcRX/m0VxPn1/VogAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMbjPkSNQFO8XwQAALcTVogAAIDxCEQAAMB4BCIAAGA8owLRypUr1a1bN7Vo0UKDBg3SgQMHfF0SAABoBIwJRO+++66SkpK0cOFCHT58WAMGDFBsbKwKCwt9XRoAAPAxYwLR0qVL9dRTT+mJJ55QVFSUVq9erVatWmnt2rW+Lg0AAPiYEZfdl5WVKSsrS/Pnz3dva9asmWJiYpSZmVmtfWlpqUpLS93Pi4uLJUkul6te6qss/b5e+gUAoKmoj8/Yqj4ty7phWyMC0T/+8Q9VVFQoLCzMY3tYWJi++OKLau2Tk5P1wgsvVNveuXPneqsRAACTBb9Wf31fuHBBwcHBNbYxIhDV1vz585WUlOR+XllZqfPnz6t9+/ay2Ww+rKz+uVwude7cWWfOnJHdbvd1OfWKsd6+TBovY719mTTe+hqrZVm6cOGCIiIibtjWiEDUoUMHNW/eXAUFBR7bCwoKFB4eXq19YGCgAgMDPbaFhITUZ4mNjt1uv+0PwCqM9fZl0ngZ6+3LpPHWx1hvtDJUxYiTqgMCAhQdHa309HT3tsrKSqWnp8vhcPiwMgAA0BgYsUIkSUlJSYqPj9c999yje++9V6+99pouXbqkJ554wtelAQAAHzMmEI0fP17nzp3TggUL5HQ6dffdd2vHjh3VTrQ2XWBgoBYuXFjtK8PbEWO9fZk0XsZ6+zJpvI1hrDbrZq5FAwAAuI0ZcQ4RAABATQhEAADAeAQiAABgPAIRAAAwHoHIIMnJyfrpT3+qoKAghYaGasyYMcrJyanxNSkpKbLZbB6PFi1aNFDF3vvDH/5Qre7evXvX+JrNmzerd+/eatGihfr166f//d//baBqb023bt2qjdVmsykxMfGa7ZvanO7du1cPPvigIiIiZLPZ9N5773nstyxLCxYsUMeOHdWyZUvFxMTo5MmTN+x35cqV6tatm1q0aKFBgwbpwIED9TSCm1fTWMvLyzV37lz169dPrVu3VkREhCZNmqS8vLwa+/TmWGgIN5rXyZMnV6t7xIgRN+y3Mc6rdOPxXusYttlsWrJkyXX7bKxzezOfNSUlJUpMTFT79u3Vpk0bjRs3rtrNk6/m7bF+swhEBsnIyFBiYqI++eQTpaWlqby8XMOHD9elS5dqfJ3dbld+fr77cfr06Qaq+NbcddddHnX/7W9/u27bffv26ZFHHlFCQoKOHDmiMWPGaMyYMTp+/HgDVuydgwcPeowzLS1NkvRv//Zv131NU5rTS5cuacCAAVq5cuU19y9evFjLly/X6tWrtX//frVu3VqxsbEqKSm5bp/vvvuukpKStHDhQh0+fFgDBgxQbGysCgsL62sYN6WmsX7//fc6fPiwfv/73+vw4cPasmWLcnJy9Ktf/eqG/dbmWGgoN5pXSRoxYoRH3e+8806NfTbWeZVuPN4fjzM/P19r166VzWbTuHHjauy3Mc7tzXzWzJo1Sx988IE2b96sjIwM5eXlaezYsTX2682xXisWjFVYWGhJsjIyMq7bZt26dVZwcHDDFVVHFi5caA0YMOCm2//617+24uLiPLYNGjTI+s1vflPHldW/GTNmWHfeeadVWVl5zf1NdU4ty7IkWVu3bnU/r6ystMLDw60lS5a4txUVFVmBgYHWO++8c91+7r33XisxMdH9vKKiwoqIiLCSk5PrpW5vXD3Wazlw4IAlyTp9+vR129T2WPCFa401Pj7eGj16dK36aQrzalk3N7ejR4+2hg4dWmObpjC3llX9s6aoqMjy9/e3Nm/e7G7z+eefW5KszMzMa/bh7bFeG6wQGay4uFiS1K5duxrbXbx4UV27dlXnzp01evRonThxoiHKu2UnT55URESEfvKTn2jixInKzc29btvMzEzFxMR4bIuNjVVmZmZ9l1mnysrK9NZbb2nKlCk1/hBxU53Tq506dUpOp9Nj7oKDgzVo0KDrzl1ZWZmysrI8XtOsWTPFxMQ0ufkuLi6WzWa74W8t1uZYaEz27Nmj0NBQ9erVS9OmTdO333573ba307wWFBQoNTVVCQkJN2zbFOb26s+arKwslZeXe8xV79691aVLl+vOlTfHem0RiAxVWVmpmTNn6r777lPfvn2v265Xr15au3at3n//fb311luqrKzUz372M509e7YBq629QYMGKSUlRTt27NCqVat06tQpPfDAA7pw4cI12zudzmp3LQ8LC5PT6WyIcuvMe++9p6KiIk2ePPm6bZrqnF5L1fzUZu7+8Y9/qKKiosnPd0lJiebOnatHHnmkxh/DrO2x0FiMGDFCGzZsUHp6ul5++WVlZGRo5MiRqqiouGb722VeJWn9+vUKCgq64VdITWFur/VZ43Q6FRAQUC3I1zRX3hzrtWXMT3fAU2Jioo4fP37D75sdDofHD+D+7Gc/U58+ffTGG2/opZdequ8yvTZy5Ej3n/v3769Bgwapa9eu2rRp0039r6upWrNmjUaOHKmIiIjrtmmqc4r/p7y8XL/+9a9lWZZWrVpVY9umeixMmDDB/ed+/fqpf//+uvPOO7Vnzx4NGzbMh5XVv7Vr12rixIk3vNihKcztzX7WNAasEBlo+vTp2rZtm3bv3q1OnTrV6rX+/v7653/+Z3311Vf1VF39CAkJUc+ePa9bd3h4eLUrHAoKChQeHt4Q5dWJ06dP66OPPtKTTz5Zq9c11TmV5J6f2sxdhw4d1Lx58yY731Vh6PTp00pLS6txdehabnQsNFY/+clP1KFDh+vW3dTntcpf//pX5eTk1Po4lhrf3F7vsyY8PFxlZWUqKiryaF/TXHlzrNcWgcgglmVp+vTp2rp1q3bt2qXIyMha91FRUaFjx46pY8eO9VBh/bl48aK+/vrr69btcDiUnp7usS0tLc1jJaWxW7dunUJDQxUXF1er1zXVOZWkyMhIhYeHe8ydy+XS/v37rzt3AQEBio6O9nhNZWWl0tPTG/18V4WhkydP6qOPPlL79u1r3ceNjoXG6uzZs/r222+vW3dTntcfW7NmjaKjozVgwIBav7axzO2NPmuio6Pl7+/vMVc5OTnKzc297lx5c6x7UzgMMW3aNCs4ONjas2ePlZ+f7358//337jaPP/64NW/ePPfzF154wdq5c6f19ddfW1lZWdaECROsFi1aWCdOnPDFEG7as88+a+3Zs8c6deqU9fHHH1sxMTFWhw4drMLCQsuyqo/z448/tvz8/Kz//M//tD7//HNr4cKFlr+/v3Xs2DFfDaFWKioqrC5dulhz586ttq+pz+mFCxesI0eOWEeOHLEkWUuXLrWOHDnivrJq0aJFVkhIiPX+++9bR48etUaPHm1FRkZaly9fdvcxdOhQ609/+pP7+caNG63AwEArJSXF+uyzz6ynn37aCgkJsZxOZ4OP78dqGmtZWZn1q1/9yurUqZOVnZ3tcQyXlpa6+7h6rDc6FnylprFeuHDBmj17tpWZmWmdOnXK+uijj6yBAwdaPXr0sEpKStx9NJV5tawb/zu2LMsqLi62WrVqZa1ateqafTSVub2Zz5qpU6daXbp0sXbt2mUdOnTIcjgclsPh8OinV69e1pYtW9zPb+ZYvxUEIoNIuuZj3bp17jb/8i//YsXHx7ufz5w50+rSpYsVEBBghYWFWaNGjbIOHz7c8MXX0vjx462OHTtaAQEB1j/90z9Z48ePt7766iv3/qvHaVmWtWnTJqtnz55WQECAddddd1mpqakNXLX3du7caUmycnJyqu1r6nO6e/fua/67rRpTZWWl9fvf/94KCwuzAgMDrWHDhlX7e+jatau1cOFCj21/+tOf3H8P9957r/XJJ5800Iiur6axnjp16rrH8O7du919XD3WGx0LvlLTWL///ntr+PDh1h133GH5+/tbXbt2tZ566qlqwaapzKtl3fjfsWVZ1htvvGG1bNnSKioqumYfTWVub+az5vLly9Zvf/tbq23btlarVq2shx56yMrPz6/Wz49fczPH+q2w/f9vCgAAYCzOIQIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeP8fwzu8XWzi/5MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "id": "17I9wrn8ZgsN"
   },
   "outputs": [],
>>>>>>> a74126e1bd138ec40f9ec59bad7be159baf0fceb
   "source": [
    "#convert rating to numeric values, and plot the histogram of values\n",
    "rating=vgr.website_rating.apply(lambda k: k[:-3])\n",
    "vgr['rating']=pd.to_numeric(rating)\n",
    "vgr.rating.plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JB1HYixvZgsO"
   },
   "source": [
    "Most games seem to have a rating between 11 and 16. In this exercise, we will try to determine if we can determine if a game is very good (rating above 16) or very bad (rating below 11) based only on the summary of its review.\n",
    "\n",
    "Let's start by splitting the dataset between good and bad games"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7554/1438388647.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bad['quality']=pd.Series([\"bad\"]*len(bad.index),index=bad.index)\n",
      "/tmp/ipykernel_7554/1438388647.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  good['quality']=pd.Series([\"good\"]*len(good.index),index=good.index)\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "id": "nZWhweYuZgsO"
   },
   "outputs": [],
>>>>>>> a74126e1bd138ec40f9ec59bad7be159baf0fceb
   "source": [
    "bad=vgr[(vgr.rating<=11) & (vgr.platform==\"PC\")]\n",
    "bad['quality']=pd.Series([\"bad\"]*len(bad.index),index=bad.index)\n",
    "good=vgr[(vgr.rating>=16) & (vgr.platform==\"PC\")]\n",
    "good['quality']=pd.Series([\"good\"]*len(good.index),index=good.index)\n",
    "selected_games=pd.concat([good,bad]).dropna()\n",
    "\n",
    "#Keep only reviews and \n",
    "game_reviews=selected_games['description']\n",
    "game_quality=selected_games['quality']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRuc6EaNZgsO"
   },
   "source": [
    "Lemmatization in French is a tricky issue.\n",
    "\n",
    "One example : the verb finir can be expressed as finissons, finirez, finisse, finit, etc...\n",
    "Lemmatization is typically less efficient in french than in english. \n",
    "\n",
    "Another alternative is to use Stemming instead. Stemming uses RegEx rules to truncate the end of a word that would normally correspond to conjugations, inflections, etc...\n",
    "Stemming destructs the readability of the words by truncating their end, but runs faster than Lemmatization\n",
    "\n",
    "In the next cell, we adapt the LemmaTokenizer that we defined earlier using a FrenchStemmer instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vA9TEMYXZgsO"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk import wordpunct_tokenize          \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from string import punctuation\n",
    "class FrenchStemTokenizer(object):\n",
    "    def __init__(self, remove_non_words=True):\n",
    "        self.st = FrenchStemmer()\n",
    "        self.stopwords = set(stopwords.words('french'))\n",
    "        self.words = set(words.words())\n",
    "        self.remove_non_words = remove_non_words\n",
    "    def __call__(self, doc):\n",
    "        # tokenize words and punctuation\n",
    "        word_list = wordpunct_tokenize(doc)\n",
    "        # remove stopwords\n",
    "        word_list = [word for word in word_list if word not in self.stopwords]\n",
    "        # remove non words\n",
    "        if(self.remove_non_words):\n",
    "            word_list = [word for word in word_list if word in self.words]\n",
    "        # remove 1-character words\n",
    "        word_list = [word for word in word_list if len(word)>1]\n",
    "        # remove non alpha\n",
    "        word_list = [word for word in word_list if word.isalpha()]\n",
    "        return [self.st.stem(t) for t in word_list]\n",
    "\n",
    "countvect = CountVectorizer(tokenizer=FrenchStemTokenizer(remove_non_words=True))\n",
    "bow_games = countvect.fit_transform(game_reviews)\n",
    "feat2word = {v: k for k, v in countvect.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eirJF0KjZgsP"
   },
   "source": [
    "### Classify with BOW"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2349\n",
      "Number of words: 3161\n",
      "Document - words matrix: (2349, 3161)\n",
      "First words: ['abandon', 'abomin', 'abord', 'abras', 'absenc', 'absolut', 'absorb', 'abstract', 'abyssal', 'academy', 'accent', 'accept', 'access', 'accessibl', 'acclaim', 'accord', 'accouch', 'accru', 'accus', 'ace', 'aci', 'acolyt', 'acquisit', 'acquit', 'acquitt', 'act', 'action', 'activ', 'activity', 'actual', 'ad', 'adag', 'adapt', 'add', 'addict', 'addit', 'adieu', 'adieux', 'admir', 'adolescent', 'adopt', 'ador', 'adrenalin', 'adroit', 'advanc', 'advanced', 'adventur', 'advers', 'aero', 'affect', 'affili', 'affirm', 'affluenc', 'afflux', 'affront', 'afraid', 'after', 'afterbirth', 'against', 'age', 'agend', 'agent', 'aggress', 'agit', 'agon', 'agricol', 'agricultur', 'ah', 'aid', 'aiguill', 'aim', 'aion', 'air', 'al', 'alan', 'album', 'alfa', 'ali', 'alien', 'align', 'aliment', 'aliv', 'all', 'allan', 'allemand', 'aller', 'allianc', 'allur', 'allus', 'almost', 'alon', 'alpha', 'alphabet', 'altern', 'am', 'amass', 'amateur', 'amatric', 'ambit', 'ambivalent']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/charles-alexis/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "id": "lbgEZqdpZgsP"
   },
   "outputs": [],
>>>>>>> a74126e1bd138ec40f9ec59bad7be159baf0fceb
   "source": [
    "print(\"Number of documents:\", len(game_reviews))\n",
    "words = countvect.get_feature_names()\n",
    "print(\"Number of words:\", len(words))\n",
    "print(\"Document - words matrix:\", bow_games.shape)\n",
    "print(\"First words:\", words[0:100])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6957446808510638\n",
      "Precision : 0.720164609053498\n",
      "Recall : 0.7\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "id": "54z4k0VpZgsP"
   },
   "outputs": [],
>>>>>>> a74126e1bd138ec40f9ec59bad7be159baf0fceb
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(bow_games,game_quality,test_size=0.2)\n",
    "\n",
    "#Fitting classifier\n",
    "lr_classifier.fit(X_train,y_train)\n",
    "\n",
    "#Testing classifier\n",
    "y_predicted = lr_classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test,y_predicted))\n",
    "print(\"Precision :\",metrics.precision_score(y_test,y_predicted,pos_label=\"good\"))\n",
    "print(\"Recall :\",metrics.recall_score(y_test,y_predicted,pos_label=\"good\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLqa6tFTZgsQ"
   },
   "source": [
    "### Classify using tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoDUgqKgZgsQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_games = TfidfTransformer().fit_transform(bow_games)\n",
    "tfidf_games.shape"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.6659574468085107\n",
      "Precision : 0.6459854014598541\n",
      "Recall : 0.7468354430379747\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "id": "bLCm-xlzZgsQ"
   },
   "outputs": [],
>>>>>>> a74126e1bd138ec40f9ec59bad7be159baf0fceb
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_games,game_quality,test_size=0.2)\n",
    "\n",
    "#Fitting classifier\n",
    "lr_classifier.fit(X_train,y_train)\n",
    "\n",
    "#Testing classifier\n",
    "y_predicted = lr_classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test,y_predicted))\n",
    "print(\"Precision :\",metrics.precision_score(y_test,y_predicted,pos_label=\"good\"))\n",
    "print(\"Recall :\",metrics.recall_score(y_test,y_predicted,pos_label=\"good\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYmgT33VZgsR"
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "up7NxbxnZgsR"
   },
   "source": [
    "Tfidf and Bow are usually very efficient features to manipulate during cassification. However, please note that their size is directly related to the size of the vocabulary of our corpus. \n",
    "\n",
    "In our current example, even when using stemming, the dimensionnality of our bow or tfidf vectors is still very high (3161). This is not maintaintable with increasing corpora sizes.\n",
    "\n",
    "In this sction, we will use the word2vec embeddings, that address this issue by proposing an architecture that learns individual representations for words in a vector space of given dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L38iGShaZgsS"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer\n",
    "from nltk import wordpunct_tokenize          \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from string import punctuation\n",
    "import unidecode\n",
    "\n",
    "class FrenchTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stopwords = set(stopwords.words('french'))\n",
    "        self.words = set(words.words())\n",
    "    def __call__(self, doc):\n",
    "        # tokenize words and punctuation\n",
    "        word_list = wordpunct_tokenize(doc)\n",
    "        # remove stopwords\n",
    "        word_list = [word for word in word_list if word not in self.stopwords]\n",
    "        # remove 1-character words\n",
    "        word_list = [word for word in word_list if len(word)>1]\n",
    "        # remove non alpha\n",
    "        word_list = [word for word in word_list if word.isalpha()]\n",
    "        return [unidecode.unidecode(t) for t in word_list]\n",
    "\n",
    "tok=FrenchTokenizer()\n",
    "\n",
    "text_for_word2vec=[tok(sent) for sent in game_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRaGsoecZgsS"
   },
   "source": [
    "The operation above will tokenize all texts by keeping stemmed tokens. Please note the following choices :\n",
    "- we have applied stemming in order to reduce the dimensionality of our feature space\n",
    "- we have removed stop words, in order to not let context be learned with it. (depending on the use case, you may want to keep them or remove them)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOWsPngIZgsS"
   },
   "source": [
    "We can now train the Word2Vec model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XliGqhWMZgsT"
   },
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[0;32m----> 3\u001b[0m model\u001b[38;5;241m=\u001b[39m\u001b[43mWord2Vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_for_word2vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmin_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword2vec.model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m w2v\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(model\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mindex2word, model\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mvectors))\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'size'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model=Word2Vec(text_for_word2vec,size=200,window=5,min_count=1, )\n",
    "model.save(\"word2vec.model\")\n",
    "w2v=dict(zip(model.wv.index2word, model.wv.vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXaVjIVMZgsT"
   },
   "source": [
    "Let's check word similarity in our trained data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {},
=======
   "metadata": {
    "id": "M0y6VW1dZgsT"
   },
>>>>>>> a74126e1bd138ec40f9ec59bad7be159baf0fceb
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=\"jeu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDKaY4dJZgsU"
   },
   "source": [
    "Let's now try again to classify our samples using these embddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {},
=======
   "metadata": {
    "id": "ZVj4nK80ZgsU"
   },
>>>>>>> a74126e1bd138ec40f9ec59bad7be159baf0fceb
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self,word2vec,dim):\n",
    "        self.word2vec=word2vec\n",
    "        self.dim=dim\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {},
=======
   "metadata": {
    "id": "i_n-EJW5ZgsV"
   },
>>>>>>> a74126e1bd138ec40f9ec59bad7be159baf0fceb
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(game_reviews,game_quality,test_size=0.2)\n",
    "\n",
    "pipe=Pipeline([('vectorizer',MeanEmbeddingVectorizer(w2v,200)),('classifier',lr_classifier)])\n",
    "\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {},
=======
   "metadata": {
    "id": "Tg9TYPRdZgsV"
   },
>>>>>>> a74126e1bd138ec40f9ec59bad7be159baf0fceb
   "outputs": [],
   "source": [
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_test,predicted))\n",
    "print(\"Precision :\",metrics.precision_score(y_test,predicted,pos_label=\"good\"))\n",
    "print(\"Recall :\",metrics.recall_score(y_test,predicted,pos_label=\"good\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNCJSWtaZgsW"
   },
   "source": [
    "What we observe here is that word2vec embeddings perform worse than what we learned from BOW or TFIDF. \n",
    "\n",
    "In our case, the training corpus for the embeddings was not large enough to ensure proper convergence and representation of the words.\n",
    "\n",
    "It is also common that for smaller corpora (<10.000 docs approximately), TFIDF usually performs better for classification, whereas Word2Vec produces better results with larger corpora and across domains (e.g. training on data from Wikipedia, and then using the vectors on data from another field)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.8.10"
=======
   "version": "3.10.8"
>>>>>>> a74126e1bd138ec40f9ec59bad7be159baf0fceb
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
